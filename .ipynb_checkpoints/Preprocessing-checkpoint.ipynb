{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessing(object):\n",
    "    \n",
    "    def __init__(self,label_abstract = [],label_intro = []):\n",
    "        \"\"\"\n",
    "        * A preprocessing object which allows perform most of the \n",
    "        * required transformations to manipulate dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        self.label_abstract = label_abstract\n",
    "        self.label_intro = label_intro\n",
    "        self.super_dict = {}\n",
    "        self.tag_feats = {}\n",
    "        \n",
    "  \n",
    "    def tokenize_section(self,section):\n",
    "        \"\"\"\n",
    "        * Splits each section into a list of sentences \n",
    "        \"\"\"\n",
    "        \n",
    "        #Split section by sentence\n",
    "        tokenized_section = section.split(\"\\n\")\n",
    "        \n",
    "        return tokenized_section\n",
    "    \n",
    "    def label_data(self,tokenized_section):\n",
    "        \"\"\"\n",
    "        * Labels each sentence of tokenized section\n",
    "        * additionally returns the tags frequencies of each\n",
    "        * of the sentences contained in the tokenized section\n",
    "        \"\"\"\n",
    "        \n",
    "        label_section = []\n",
    "        tags_freq_section = []\n",
    "        \n",
    "        for sentence in tokenized_section:\n",
    "            \n",
    "            sentence_tagged,tags_freq = self.preprocess_sentence(sentence[5:]) \n",
    "            preproc_sentence = (sentence_tagged, sentence[:4])\n",
    "            \n",
    "            tags_freq_section.append(tags_freq)\n",
    "            label_section.append(preproc_sentence)\n",
    "            \n",
    "        return label_section, tags_freq_section\n",
    "\n",
    "    \n",
    "    def preprocess_sentence(self,sentence):\n",
    "        \"\"\"\n",
    "        * Lowercases a given sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        lowered_sentence = []\n",
    "        sentence_tokenized = self.tokenize_sentence(sentence)\n",
    "        \n",
    "        for word in sentence_tokenized:\n",
    "            \n",
    "            if not word.isupper():\n",
    "                lowered_sentence.append(word.lower())\n",
    "\n",
    "            else:\n",
    "                lowered_sentence.append(word)\n",
    "        \n",
    "        sentence_tagged,tags_freq = self.tagging_data(lowered_sentence)\n",
    "                \n",
    "        return sentence_tagged,tags_freq\n",
    "    \n",
    "    def tokenize_sentence(self,sentence):\n",
    "        \"\"\"\n",
    "        * Tokenize a given sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        sentence_tokenized = nltk.wordpunct_tokenize(sentence)\n",
    "        return sentence_tokenized\n",
    "    \n",
    "    \n",
    "    def tagging_data(self,lowered_sentence):\n",
    "        \"\"\"\n",
    "        * Performs all tag-related tasks to the lowered_sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        sentence_tagged = self.pos_tag_sentence(lowered_sentence)\n",
    "        tags_freq = self.tag_counter(sentence_tagged)\n",
    "        \n",
    "        return sentence_tagged,tags_freq\n",
    "    \n",
    "    def pos_tag_sentence(self,lowered_sentence):\n",
    "        \"\"\"\n",
    "        * Performs the POS_tag of the lowered_sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        sentence_tagged = nltk.pos_tag(lowered_sentence)\n",
    "        \n",
    "        return sentence_tagged\n",
    "    \n",
    "    def tag_counter(self,sentence_tagged):\n",
    "        \"\"\"\n",
    "        * Counts the tag distribution of a sentence_tagged\n",
    "        \"\"\"\n",
    "        \n",
    "        tags_freq = {}\n",
    "        \n",
    "        for token in sentence_tagged:\n",
    "            if token[1] in tags_freq:\n",
    "                tags_freq[token[1]] +=1\n",
    "            else:\n",
    "                tags_freq.update({token[1]:1})\n",
    "                    \n",
    "        return tags_freq\n",
    "            \n",
    "    def sum_all_tags(self,list_tag_freqs):\n",
    "        \"\"\"\n",
    "        * Aggregates the distinct tags of list_tag_freqs\n",
    "        * into a unique dictionary\n",
    "        \"\"\"\n",
    "        \n",
    "        for set_tag_freqs in list_tag_freqs:\n",
    "            for dictlist in set_tag_freqs:\n",
    "                for dic in dictlist:\n",
    "                    for k,v in dic.items():\n",
    "                        if k in self.super_dict:\n",
    "                            self.super_dict[k] += v\n",
    "                        else:\n",
    "                            self.super_dict.update({k:v})\n",
    "                            \n",
    "        return self.super_dict\n",
    "    \n",
    "    def reset_all_tags(self):\n",
    "        \"\"\"\n",
    "        * Resets the values of all tags without removing the keys\n",
    "        * of the dictionary involved\n",
    "        \"\"\"\n",
    "        \n",
    "        additional_feats = ['abstract','introduction','paper','unknown']\n",
    "        \n",
    "        self.tag_feats = dict((k,0)for k in self.super_dict)\n",
    "        \n",
    "        for feat in additional_feats:\n",
    "            self.tag_feats.update({feat:0})\n",
    "            \n",
    "        return \n",
    "    \n",
    "    def build_feature_set(self,label_section,sectionName):\n",
    "        \"\"\"\n",
    "        * Builds the feature set of each sentence contained in the \n",
    "        * label_section\n",
    "        \"\"\"\n",
    "        \n",
    "        section_dataset = []\n",
    "        for doc in label_section:\n",
    "            for sentence in doc:\n",
    "                \n",
    "                feature_set,sentence = self.feature_freq(sentence)\n",
    "                \n",
    "                feats_sentence = self.build_sentence_features(feature_set[0])\n",
    "                \n",
    "                \n",
    "                if sectionName == \"abstract\":\n",
    "                    feats_sentence[sectionName] = 1\n",
    "\n",
    "                elif sectionName == \"introduction\":\n",
    "                    feats_sentence[sectionName] = 1\n",
    "                \n",
    "                \n",
    "                section_set = ((feats_sentence,feature_set[1]),sentence)\n",
    "                section_dataset.append(section_set)\n",
    "                \n",
    "        return section_dataset\n",
    "                \n",
    "     \n",
    "    def feature_freq(self,sentence):\n",
    "        \"\"\"\n",
    "        * Pairs a label with its corresponding \n",
    "        * tag distribution of a sentence_tagged\n",
    "        \"\"\"\n",
    "        \n",
    "        tags_freq = {}\n",
    "        \n",
    "        for token in sentence[0]:\n",
    "            \n",
    "            if token[1] in tags_freq:\n",
    "                tags_freq[token[1]] +=1\n",
    "            else:\n",
    "                tags_freq.update({token[1]:1})\n",
    "            \n",
    "        return (tags_freq,sentence[1]),sentence\n",
    "            \n",
    "    def build_sentence_features(self,feature_set):  \n",
    "        \"\"\"\n",
    "        * Maps values of feature_set into the form of the unique feature_set\n",
    "        \"\"\"\n",
    "        \n",
    "        self.reset_all_tags()\n",
    "        \n",
    "        \n",
    "        for k,v in feature_set.items():\n",
    "            if k in self.tag_feats:\n",
    "                self.tag_feats[k] += v\n",
    "            else:\n",
    "                self.tag_feats['unknown'] += v\n",
    "        \n",
    "        return self.tag_feats\n",
    "    \n",
    "    def split_dataset(self,feats_abs,feats_intro,size=0.3,devSize = 0.15):\n",
    "        \"\"\"\n",
    "        * Splits the dataset into a training,testing and dev set\n",
    "        \"\"\"\n",
    "        \n",
    "        train_set = []\n",
    "        dev_set = []\n",
    "        test_set = []\n",
    "        \n",
    "        shuffle(feats_abs)\n",
    "        shuffle(feats_intro)\n",
    "        \n",
    "        thres_train =  int(len(feats_abs)*size)\n",
    "        thres_dev =  int(len(feats_abs)*devSize) \n",
    "        \n",
    "        tmp_abs_train = feats_abs[0:thres_train]\n",
    "        tmp_dev_train = tmp_abs_train[-thres_dev:]\n",
    "        tmp_abs_test = feats_abs[thres_train:]\n",
    "        \n",
    "        self.split_section(thres_train,thres_dev,feats_abs)\n",
    "        \n",
    "        tmp_intro_train = feats_intro[0:thres_train]\n",
    "        tmp_dev_test = tmp_intro_train[-thres_dev:]\n",
    "        tmp_intro_test = feats_intro[thres_train:]\n",
    "        \n",
    "        train_set = tmp_abs_train + tmp_intro_train\n",
    "        dev_set = tmp_dev_train + tmp_dev_test\n",
    "        test_set = tmp_abs_test + tmp_intro_test\n",
    "        \n",
    "        return train_set,dev_set,test_set\n",
    "    \n",
    "    def split_section(self,thres_train,thres_dev,feats_section):\n",
    "        \"\"\"\n",
    "        * Splits section into a training,testing and dev set\n",
    "        \"\"\"\n",
    "        \n",
    "        tmp_sec_train = feats_section[0:thres_train]\n",
    "        tmp_dev_train = tmp_sec_train[-thres_dev:]\n",
    "        tmp_sec_test = feats_section[thres_train:]\n",
    "        \n",
    "        return tmp_sec_train,tmp_dev_train,tmp_sec_test\n",
    "        \n",
    "    def extract_feats(self,train_set,dev_set,test_set):\n",
    "        \"\"\"\n",
    "        * Extract feature set from the training,testing and dev corpora\n",
    "        \"\"\"\n",
    "        \n",
    "        train_data = self.extract_feat_set(train_set)\n",
    "        test_data = self.extract_feat_set(test_set)\n",
    "        dev_data = self.extract_feat_set(test_set)\n",
    "        \n",
    "        return train_data, test_data, dev_data\n",
    "    \n",
    "    def extract_feat_set(self,feature_set):\n",
    "        \"\"\"\n",
    "        * Extracts feature set from the feature_set\n",
    "        \"\"\"\n",
    "        \n",
    "        feat_set = [feats[0] for feats in feature_set]\n",
    "        \n",
    "        return feat_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
